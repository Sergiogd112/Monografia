{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "id": "yCeMlobe8yRH",
    "outputId": "723d0a81-15ef-480a-cedc-3a8548df6345"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lime\n",
      "  Downloading https://files.pythonhosted.org/packages/e5/72/4be533df5151fcb48942515e95e88281ec439396c48d67d3ae41f27586f0/lime-0.1.1.36.tar.gz (275kB)\n",
      "\u001b[K    100% |████████████████████████████████| 276kB 1.7MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting numpy (from lime)\n",
      "  Using cached https://files.pythonhosted.org/packages/d7/b1/3367ea1f372957f97a6752ec725b87886e12af1415216feec9067e31df70/numpy-1.16.5-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting scipy (from lime)\n",
      "  Downloading https://files.pythonhosted.org/packages/1d/f6/7c16d60aeb3694e5611976cb4f1eaf1c6b7f1e7c55771d691013405a02ea/scipy-1.2.2-cp27-cp27mu-manylinux1_x86_64.whl (24.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 24.8MB 53kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting scikit-learn>=0.18 (from lime)\n",
      "  Downloading https://files.pythonhosted.org/packages/31/9f/042db462417451e81035c3d43b722e88450c628a33dfda69777a801b0d40/scikit_learn-0.20.4-cp27-cp27mu-manylinux1_x86_64.whl (5.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.5MB 233kB/s ta 0:00:011\n",
      "\u001b[?25hCollecting matplotlib==2.1.0 (from lime)\n",
      "  Downloading https://files.pythonhosted.org/packages/89/b2/41ae0d5922a8018c1f506ef9b9363fca60326c8b24241b13ecaf80fede47/matplotlib-2.1.0-cp27-cp27mu-manylinux1_x86_64.whl (14.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 14.9MB 90kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-image<0.15 (from lime)\n",
      "  Downloading https://files.pythonhosted.org/packages/bb/7f/d27fadff2c2b8cd45fffe44330b3f26014e94e856d4b61e5bfd2701b1ccb/scikit_image-0.14.5-cp27-cp27mu-manylinux1_x86_64.whl (25.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 25.5MB 55kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10 (from matplotlib==2.1.0->lime)\n",
      "  Using cached https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\n",
      "Collecting python-dateutil>=2.0 (from matplotlib==2.1.0->lime)\n",
      "  Using cached https://files.pythonhosted.org/packages/41/17/c62faccbfbd163c7f57f3844689e3a78bae1f403648a6afb1d0866d87fbb/python_dateutil-2.8.0-py2.py3-none-any.whl\n",
      "Collecting backports.functools-lru-cache (from matplotlib==2.1.0->lime)\n",
      "  Using cached https://files.pythonhosted.org/packages/03/8e/2424c0e65c4a066e28f539364deee49b6451f8fcd4f718fefa50cc3dcf48/backports.functools_lru_cache-1.5-py2.py3-none-any.whl\n",
      "Collecting subprocess32 (from matplotlib==2.1.0->lime)\n",
      "Collecting pytz (from matplotlib==2.1.0->lime)\n",
      "  Using cached https://files.pythonhosted.org/packages/87/76/46d697698a143e05f77bec5a526bf4e56a0be61d63425b68f4ba553b51f2/pytz-2019.2-py2.py3-none-any.whl\n",
      "Collecting six>=1.10 (from matplotlib==2.1.0->lime)\n",
      "  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib==2.1.0->lime)\n",
      "  Using cached https://files.pythonhosted.org/packages/11/fa/0160cd525c62d7abd076a070ff02b2b94de589f1a9789774f17d7c54058e/pyparsing-2.4.2-py2.py3-none-any.whl\n",
      "Collecting PyWavelets>=0.4.0 (from scikit-image<0.15->lime)\n",
      "  Downloading https://files.pythonhosted.org/packages/fa/f1/81d3ba0b461699a5e0dbd1a1c2c98c0b5a2e1757b7a54d49246e0a557aea/PyWavelets-1.0.3-cp27-cp27mu-manylinux1_x86_64.whl (4.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 4.4MB 322kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx>=1.8 (from scikit-image<0.15->lime)\n",
      "  Downloading https://files.pythonhosted.org/packages/f3/f4/7e20ef40b118478191cec0b58c3192f822cace858c19505c7670961b76b2/networkx-2.2.zip (1.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.7MB 667kB/s ta 0:00:011\n",
      "\u001b[?25hCollecting pillow>=4.3.0 (from scikit-image<0.15->lime)\n",
      "  Downloading https://files.pythonhosted.org/packages/cc/a4/79b5f36d1e1a2b426073bd62217d1530fcd939950c2936651e6b39127a9b/Pillow-6.1.0-cp27-cp27mu-manylinux1_x86_64.whl (2.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.1MB 584kB/s ta 0:00:011\n",
      "\u001b[?25hCollecting cloudpickle>=0.2.1 (from scikit-image<0.15->lime)\n",
      "  Downloading https://files.pythonhosted.org/packages/09/f4/4a080c349c1680a2086196fcf0286a65931708156f39568ed7051e42ff6a/cloudpickle-1.2.1-py2.py3-none-any.whl\n",
      "Collecting decorator>=4.3.0 (from networkx>=1.8->scikit-image<0.15->lime)\n",
      "  Using cached https://files.pythonhosted.org/packages/5f/88/0075e461560a1e750a0dcbf77f1d9de775028c37a19a346a6c565a257399/decorator-4.4.0-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: lime, networkx\n",
      "  Running setup.py bdist_wheel for lime ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/teo/.cache/pip/wheels/a9/2f/25/4b2127822af5761dab9a27be52e175105772aebbcbc484fb95\n",
      "  Running setup.py bdist_wheel for networkx ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/teo/.cache/pip/wheels/68/f8/29/b53346a112a07d30a5a84d53f19aeadaa1a474897c0423af91\n",
      "Successfully built lime networkx\n",
      "Installing collected packages: numpy, scipy, scikit-learn, six, cycler, python-dateutil, backports.functools-lru-cache, subprocess32, pytz, pyparsing, matplotlib, PyWavelets, decorator, networkx, pillow, cloudpickle, scikit-image, lime\n",
      "Successfully installed PyWavelets-1.0.3 backports.functools-lru-cache-1.5 cloudpickle-1.2.1 cycler-0.10.0 decorator-4.4.0 lime-0.1.1.36 matplotlib-2.2.4 networkx-2.2 numpy-1.16.5 pillow-6.1.0 pyparsing-2.4.2 python-dateutil-2.8.0 pytz-2019.2 scikit-image-0.14.5 scikit-learn-0.20.4 scipy-1.2.2 six-1.12.0 subprocess32-3.5.4\n"
     ]
    }
   ],
   "source": [
    "!pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "idU5cmRW8ucI",
    "outputId": "b2e714da-7692-424c-fb64-cb6bea76f80a"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-868e8e39ff25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreuters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import reuters\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import lime\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 957
    },
    "colab_type": "code",
    "id": "efRpvZGxXciV",
    "outputId": "5d0dc215-d62d-4e51-a769-bd44acb78c0b"
   },
   "outputs": [],
   "source": [
    "max_words = 100000\n",
    "batch_size = 32\n",
    "epochs = 9\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,\n",
    "                                                         test_split=0.2)\n",
    "X_train,X_test=x_train,x_test\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "print(num_classes, 'classes')\n",
    "\n",
    "print('Vectorizing sequence data...')\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Convert class vector to binary class matrix '\n",
    "      '(for use with categorical_crossentropy)')\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "print('Building model...')\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "gc.collect()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "print('_'*200)\n",
    "gc.collect()\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rvDJDc5M8-iv"
   },
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gHR2tQgoJOTE",
    "outputId": "a1ffd747-18c8-49e8-f37e-2e2d745c4ce9"
   },
   "outputs": [],
   "source": [
    "np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1BK87m1WFSh6",
    "outputId": "44291d7a-d90d-4590-9a7f-c6f12d07bf61"
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GwtF38jeOQwz"
   },
   "outputs": [],
   "source": [
    "def decode(prediction,labels,real):\n",
    "    x=0\n",
    "    out=[]\n",
    "    for pred in prediction:\n",
    "        decoded=[None]*len(labels)\n",
    "        for i in range(len(labels)):\n",
    "            decoded[i]=[labels[i],pred[i],real[x][i]]\n",
    "            if(real[x][i]==1):\n",
    "                out.append([labels[i],pred[i]])\n",
    "        x=x+1\n",
    "    return decoded,out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sk8Q6fCmHJ3z"
   },
   "outputs": [],
   "source": [
    "labels=['cocoa','grain','veg-oil','earn','acq','wheat','copper','housing','money-supply',\n",
    "              'coffee','sugar','trade','reserves','ship','cotton','carcass','crude','nat-gas',\n",
    "              'cpi','money-fx','interest','gnp','meal-feed','alum','oilseed','gold','tin',\n",
    "              'strategic-metal','livestock','retail','ipi','iron-steel','rubber','heat','jobs',\n",
    "              'lei','bop','zinc','orange','pet-chem','dlr','gas','silver','wpi','hog','lead']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yx7rifcuQJi1"
   },
   "outputs": [],
   "source": [
    "pred=model.predict(x=x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tt3EW8BWoaK7"
   },
   "outputs": [],
   "source": [
    "def sortbySecond(val): \n",
    "    return val[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "colab_type": "code",
    "id": "ROPgmDbbPuKH",
    "outputId": "dc689c70-a69b-4703-a5eb-b34a291a08bc"
   },
   "outputs": [],
   "source": [
    "a,b=decode(pred,labels,y_train)\n",
    "a.sort(key=sortbySecond,reverse=True)\n",
    "data=[x[1] for x in a]\n",
    "labels=[x[0] for x in a]\n",
    "real=[x[2] for x in a]\n",
    "plt.figure(dpi=75,figsize=(50,9))\n",
    "print(a[0])\n",
    "plt.bar(height=real,x=labels,width=0.9)\n",
    "plt.bar(height=data,x=labels,width=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "r6qOyyqFs1Eo",
    "outputId": "4d0b2f13-b140-499c-eb66-624f5b287051"
   },
   "outputs": [],
   "source": [
    "word_index = reuters.get_word_index()\n",
    "index_to_words={}\n",
    "for key, value in word_index.items():\n",
    "  index_to_words[value]=key\n",
    "\n",
    "print(' '.join([index_to_words[x]for x in X_train[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C64XHhjlQyfH"
   },
   "outputs": [],
   "source": [
    "class LimeAdapter():\n",
    "    @staticmethod\n",
    "    def reverse_dict(dictt):\n",
    "        index_to_words={}\n",
    "        for key, value in dictt.items():\n",
    "          index_to_words[value]=key\n",
    "        return index_to_words\n",
    "    \n",
    "    def __init__(self,model,dictionary,max_words):\n",
    "        self.model=model\n",
    "        self.dict=dictionary\n",
    "        self.max_words=max_words\n",
    "        self.tokenizer = Tokenizer(num_words=self.max_words)\n",
    "        self.reversedict=LimeAdapter.reverse_dict(dictionary)\n",
    "    \n",
    "    def text_transformer(self,text):\n",
    "        data=text_to_num(text)\n",
    "        \n",
    "        data = self.tokenizer.sequences_to_matrix(data, mode='binary')\n",
    "        return data\n",
    "    def text_to_num(self,text):\n",
    "        text=text.split(' ')\n",
    "        data=[self.dict[x] for x in text]\n",
    "        return data\n",
    "    def num_to_text(self,matrix):\n",
    "        data=self.tokenizer.matrix_to_sequence(matrix,mode='binary')\n",
    "        text=' '.join([self.reversedict[x] for x in data])\n",
    "        return text\n",
    "    def predict(self,text):\n",
    "        data=self.textransformer(text)\n",
    "        a=self.model.predict(data)\n",
    "        return a        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cgVzFaAdy0bT"
   },
   "outputs": [],
   "source": [
    "adapter=LimeAdapter(model,word_index,max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G_NODEtNXbAF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T75EOmvEY55H"
   },
   "outputs": [],
   "source": [
    "explainer=LimeTextExplainer(class_names=labels)\n",
    "explainer.explain_instance(X_test,model.predict,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "utD0qpCGN5gv"
   },
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OKd2SyOSbbcy"
   },
   "outputs": [],
   "source": [
    "# IMPORT MODULES\n",
    "# TURN ON the GPU !!!\n",
    "# If importing dataset from outside - like this IMDB - Internet must be \"connected\"\n",
    "\n",
    "import os\n",
    "from operator import itemgetter    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "get_ipython().magic(u'matplotlib inline')\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import models, regularizers, layers, optimizers, losses, metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils, to_categorical\n",
    " \n",
    "from keras.datasets import reuters\n",
    "\n",
    "print(os.getcwd())\n",
    "print(\"Modules imported \\n\")\n",
    "print(\"Files in current directory:\")\n",
    "from subprocess import check_output\n",
    "# print(check_output([\"ls\", \"../input\"]).decode(\"utf8\")) #check the files available in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9JThU1fncez3"
   },
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E5mGr3tyci5H"
   },
   "outputs": [],
   "source": [
    "print(\"train_data \", train_data.shape)\n",
    "print(\"train_labels \", train_labels.shape)\n",
    "\n",
    "print(\"test_data \", test_data.shape)\n",
    "print(\"test_labels \", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6KM-zdzhcm6j"
   },
   "outputs": [],
   "source": [
    "# Reverse dictionary to see words instead of integers\n",
    "# Note that the indices are offset by 3 because 0, 1, and 2 are reserved indices for “padding,” “start of sequence,” and “unknown.”\n",
    "\n",
    "word_index = reuters.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in\n",
    "train_data[0]])\n",
    "\n",
    "print(decoded_newswire)\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ijSyRRxAczu0"
   },
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4xlAt8Dlc1hI"
   },
   "outputs": [],
   "source": [
    "# Vectorize and Normalize train and test to tensors with 10k columns\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n",
    "\n",
    "print(\"x_train \", x_train.shape)\n",
    "print(\"x_test \", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MQa3umL_c7pM"
   },
   "outputs": [],
   "source": [
    "# ONE HOT ENCODER of the labels\n",
    "\n",
    "one_hot_train_labels = to_categorical(train_labels)\n",
    "one_hot_test_labels = to_categorical(test_labels)\n",
    "\n",
    "print(\"one_hot_train_labels \", one_hot_train_labels.shape)\n",
    "print(\"one_hot_test_labels \", one_hot_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tRpOnSW1dAUl"
   },
   "outputs": [],
   "source": [
    "# Setting aside a VALIDATION set\n",
    "\n",
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]\n",
    "\n",
    "print(\"x_val \", x_val.shape)\n",
    "print(\"y_val \", y_val.shape)\n",
    "\n",
    "print(\"partial_x_train \", partial_x_train.shape)\n",
    "print(\"partial_y_train \", partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B7XvJS8BdFMm"
   },
   "outputs": [],
   "source": [
    "# MODEL\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, kernel_regularizer=regularizers.l1(0.001), activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(256, kernel_regularizer=regularizers.l1(0.001), activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(256, kernel_regularizer=regularizers.l1(0.001), activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "# REGULARIZERS L1 L2\n",
    "#regularizers.l1(0.001)\n",
    "#regularizers.l2(0.001)\n",
    "#regularizers.l1_l2(l1=0.001, l2=0.001)\n",
    "\n",
    "# Best results I got with HU=128/128/128 or 256/256 and L1=0.001 and Dropout=0.5 = 77.02%\n",
    "# Without Regularizer 72.92%\n",
    "# Reg L1 = 76.04, L2 = 76.2, L1_L2 = 76.0\n",
    "# Only DropOut (0.5) = 76.85%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rbcjthWTdLFJ"
   },
   "outputs": [],
   "source": [
    "# FIT / TRAIN model\n",
    "\n",
    "NumEpochs = 100\n",
    "BatchSize = 512\n",
    "\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(partial_x_train, partial_y_train, epochs=NumEpochs, batch_size=BatchSize, validation_data=(x_val, y_val))\n",
    "\n",
    "results = model.evaluate(x_val, y_val)\n",
    "print(\"_\"*100)\n",
    "print(\"Test Loss and Accuracy\")\n",
    "print(\"results \", results)\n",
    "\n",
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "39BrPMlBdVis"
   },
   "outputs": [],
   "source": [
    "# VALIDATION LOSS curves\n",
    "\n",
    "plt.clf()\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CZQUgXo5daKE"
   },
   "outputs": [],
   "source": [
    "## VALIDATION ACCURACY curves\n",
    "\n",
    "plt.clf()\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aV1pM3Gwdd-O"
   },
   "outputs": [],
   "source": [
    "# Retrain from scratch for # of epochs per LEARNING curves above - and evaluate with TEST (which was set aside above)\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(partial_x_train,partial_y_train,epochs= NumEpochs, batch_size=512,\n",
    "validation_data=(x_val, y_val))\n",
    "\n",
    "results = model.evaluate(x_test, one_hot_test_labels)\n",
    "\n",
    "print(\"_\"*200)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mQmsldf1dmSy"
   },
   "outputs": [],
   "source": [
    "# PREDICT\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "# Each entry in predictions is a vector of length 46\n",
    "print(predictions[123].shape)\n",
    "\n",
    "# The coefficients in this vector sum to 1:\n",
    "print(np.sum(predictions[123]))\n",
    "\n",
    "# The largest entry is the predicted class — the class with the highest probability:\n",
    "print(np.argmax(predictions[123]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Anntextreuters.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
