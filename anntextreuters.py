# -*- coding: utf-8 -*-
"""Anntextreuters.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nzFg3vWPW_WCpphNbH8xuKTny3rzOW6j
"""

!pip install lime

from __future__ import print_function

import numpy as np
import keras
from keras.datasets import reuters
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.preprocessing.text import Tokenizer
from sklearn.metrics import accuracy_score, classification_report

import lime

import gc

max_words = 100000
batch_size = 32
epochs = 9

print('Loading data...')
(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,
                                                         test_split=0.2)
X_train,X_test=x_train,x_test
print(len(x_train), 'train sequences')
print(len(x_test), 'test sequences')

num_classes = np.max(y_train) + 1
print(num_classes, 'classes')

print('Vectorizing sequence data...')
tokenizer = Tokenizer(num_words=max_words)
x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')
x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')
print('x_train shape:', x_train.shape)
print('x_test shape:', x_test.shape)

print('Convert class vector to binary class matrix '
      '(for use with categorical_crossentropy)')
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)
print('y_train shape:', y_train.shape)
print('y_test shape:', y_test.shape)

print('Building model...')
model = Sequential()
model.add(Dense(512, input_shape=(max_words,)))
model.add(Activation('sigmoid'))
model.add(Dropout(0.5))
model.add(Dense(512, input_shape=(max_words,)))
model.add(Activation('sigmoid'))
model.add(Dropout(0.5))
model.add(Dense(num_classes))
model.add(Activation('softmax'))
gc.collect()
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

history = model.fit(x_train, y_train,
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=1,
                    validation_split=0.1)
print('_'*200)
gc.collect()
score = model.evaluate(x_test, y_test,
                       batch_size=batch_size, verbose=1)
print('Test score:', score[0])
print('Test accuracy:', score[1])
gc.collect()

from lime.lime_text import LimeTextExplainer
from matplotlib import pyplot as plt

np.shape(y_train)

gc.collect()

def decode(prediction,labels,real):
    x=0
    out=[]
    for pred in prediction:
        decoded=[None]*len(labels)
        for i in range(len(labels)):
            decoded[i]=[labels[i],pred[i],real[x][i]]
            if(real[x][i]==1):
                out.append([labels[i],pred[i]])
        x=x+1
    return decoded,out

labels=['cocoa','grain','veg-oil','earn','acq','wheat','copper','housing','money-supply',
              'coffee','sugar','trade','reserves','ship','cotton','carcass','crude','nat-gas',
              'cpi','money-fx','interest','gnp','meal-feed','alum','oilseed','gold','tin',
              'strategic-metal','livestock','retail','ipi','iron-steel','rubber','heat','jobs',
              'lei','bop','zinc','orange','pet-chem','dlr','gas','silver','wpi','hog','lead']

pred=model.predict(x=x_train)

def sortbySecond(val): 
    return val[1]

a,b=decode(pred,labels,y_train)
a.sort(key=sortbySecond,reverse=True)
data=[x[1] for x in a]
labels=[x[0] for x in a]
real=[x[2] for x in a]
plt.figure(dpi=75,figsize=(50,9))
print(a[0])
plt.bar(height=real,x=labels,width=0.9)
plt.bar(height=data,x=labels,width=0.9)

word_index = reuters.get_word_index()
index_to_words={}
for key, value in word_index.items():
  index_to_words[value]=key

print(' '.join([index_to_words[x]for x in X_train[0]]))

class Mymodel():
    def __init__(self,model,dictionary,max_words):
        this.model=model
        this.dict=dictionary
        this.max_words=max_words
        this.tokenizer = Tokenizer(num_words=this.max_words)
        this.reversedict=reversedict(this.dict)
    def reversedict(dictt):
        index_to_words={}
        for key, value in dictt.items():
          index_to_words[value]=key
        return index_to_words
    def text_transformer(self,text):
        data=text_to_num(text)
        
        data = this.tokenizer.sequences_to_matrix(data, mode='binary')
        return data
    def text_to_num(self,text):
        text=text.split(' ')
        data=[this.dict[x] for x in text]
        return data
    def num_to_text(self,matrix):
        data=this.tokenizer.
        text=[]
    def predict(self,text):
        
        a=this.model.predict(data)
        return a

explainer=LimeTextExplainer(class_names=labels)
explainer.explain_instance(X_test,model.predict,)

len(labels)

# IMPORT MODULES
# TURN ON the GPU !!!
# If importing dataset from outside - like this IMDB - Internet must be "connected"

import os
from operator import itemgetter    
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')
get_ipython().magic(u'matplotlib inline')
plt.style.use('ggplot')

import tensorflow as tf

from keras import models, regularizers, layers, optimizers, losses, metrics
from keras.models import Sequential
from keras.layers import Dense
from keras.utils import np_utils, to_categorical
 
from keras.datasets import reuters

print(os.getcwd())
print("Modules imported \n")
print("Files in current directory:")
from subprocess import check_output
# print(check_output(["ls", "../input"]).decode("utf8")) #check the files available in the directory

(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

print("train_data ", train_data.shape)
print("train_labels ", train_labels.shape)

print("test_data ", test_data.shape)
print("test_labels ", test_labels.shape)

# Reverse dictionary to see words instead of integers
# Note that the indices are offset by 3 because 0, 1, and 2 are reserved indices for “padding,” “start of sequence,” and “unknown.”

word_index = reuters.get_word_index()
reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in
train_data[0]])

print(decoded_newswire)
print(train_labels[0])

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

# Vectorize and Normalize train and test to tensors with 10k columns

x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)

print("x_train ", x_train.shape)
print("x_test ", x_test.shape)

# ONE HOT ENCODER of the labels

one_hot_train_labels = to_categorical(train_labels)
one_hot_test_labels = to_categorical(test_labels)

print("one_hot_train_labels ", one_hot_train_labels.shape)
print("one_hot_test_labels ", one_hot_test_labels.shape)

# Setting aside a VALIDATION set

x_val = x_train[:1000]
partial_x_train = x_train[1000:]
y_val = one_hot_train_labels[:1000]
partial_y_train = one_hot_train_labels[1000:]

print("x_val ", x_val.shape)
print("y_val ", y_val.shape)

print("partial_x_train ", partial_x_train.shape)
print("partial_y_train ", partial_y_train.shape)

# MODEL

model = models.Sequential()
model.add(layers.Dense(256, kernel_regularizer=regularizers.l1(0.001), activation='relu', input_shape=(10000,)))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(256, kernel_regularizer=regularizers.l1(0.001), activation='relu'))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(256, kernel_regularizer=regularizers.l1(0.001), activation='relu'))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(46, activation='softmax'))

# REGULARIZERS L1 L2
#regularizers.l1(0.001)
#regularizers.l2(0.001)
#regularizers.l1_l2(l1=0.001, l2=0.001)

# Best results I got with HU=128/128/128 or 256/256 and L1=0.001 and Dropout=0.5 = 77.02%
# Without Regularizer 72.92%
# Reg L1 = 76.04, L2 = 76.2, L1_L2 = 76.0
# Only DropOut (0.5) = 76.85%

# FIT / TRAIN model

NumEpochs = 100
BatchSize = 512

model.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])
history = model.fit(partial_x_train, partial_y_train, epochs=NumEpochs, batch_size=BatchSize, validation_data=(x_val, y_val))

results = model.evaluate(x_val, y_val)
print("_"*100)
print("Test Loss and Accuracy")
print("results ", results)

history_dict = history.history
history_dict.keys()

# VALIDATION LOSS curves

plt.clf()
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'r', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

## VALIDATION ACCURACY curves

plt.clf()
acc = history.history['acc']
val_acc = history.history['val_acc']
plt.plot(epochs, acc, 'r', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Retrain from scratch for # of epochs per LEARNING curves above - and evaluate with TEST (which was set aside above)

model = models.Sequential()
model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(46, activation='softmax'))

model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])
model.fit(partial_x_train,partial_y_train,epochs= NumEpochs, batch_size=512,
validation_data=(x_val, y_val))

results = model.evaluate(x_test, one_hot_test_labels)

print("_"*200)
print(results)

# PREDICT

predictions = model.predict(x_test)
# Each entry in predictions is a vector of length 46
print(predictions[123].shape)

# The coefficients in this vector sum to 1:
print(np.sum(predictions[123]))

# The largest entry is the predicted class — the class with the highest probability:
print(np.argmax(predictions[123]))